{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Downloading necessary NLTK data\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords'])\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "### Importing libraries\n",
    "from workspace_utils import active_session\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading data from the database\n",
    "querystring = \"\"\"SELECT * from disaster_response_df\"\"\"\n",
    "engine = create_engine('sqlite:///InsertDatabaseName.db')\n",
    "\n",
    "df = pd.read_sql(querystring, engine)\n",
    "\n",
    "### Dropping columns and converting to Numpy arrays\n",
    "X = df[['message']].values\n",
    "y = df.drop(['id', 'message', 'original', 'genre'], axis=1).values\n",
    "#y['related'] = y['related'].map(lambda x: 1 if x ==2 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('disaster_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \n",
    "    ### Getting list of all urls using regex\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    \n",
    "    ### Replacing each url in text string with placeholder\n",
    "    for url in detected_urls:\n",
    "        bbtext = text.replace(url, \"urlplaceholder\")\n",
    "    \n",
    "    ### Tokenizing text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    ### Initiating lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "    ### Lemmatizing, normalizing case, and removing leading/trailing white space and stop words\n",
    "    return[lemmatizer.lemmatize(tok).lower().strip() for tok in tokens if tok not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is the Hurricane over or is it not over'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Tokenizing a row\n",
    "s = str(X[1]).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'hurricane']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing the tokenization function\n",
    "tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing packages\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Building the Pipeline\n",
    "pipeline = Pipeline([('cvectorizer', CountVectorizer(tokenizer=tokenize)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('rf_clf', RandomForestClassifier())\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Performing train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "### Flattening arrays\n",
    "X_train = X_train.ravel()\n",
    "X_test = X_test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "      ...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Fitting the model\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9244517670636297\n",
      "Precision: 0.41621282539859356\n",
      "Recall: 0.2467297984450422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = (y_pred == y_test).mean() \n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted')}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.77      0.84      0.81      6546\n",
      "               request       0.40      0.07      0.12      1455\n",
      "                 offer       0.00      0.00      0.00        34\n",
      "           aid_related       0.43      0.15      0.22      3516\n",
      "          medical_help       0.06      0.00      0.00       665\n",
      "      medical_products       0.11      0.00      0.00       407\n",
      "     search_and_rescue       0.33      0.00      0.01       234\n",
      "              security       0.00      0.00      0.00       170\n",
      "              military       0.50      0.00      0.01       283\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.00      0.00      0.00       522\n",
      "                  food       0.16      0.01      0.01       909\n",
      "               shelter       0.06      0.00      0.00       784\n",
      "              clothing       0.00      0.00      0.00       136\n",
      "                 money       0.14      0.01      0.01       197\n",
      "        missing_people       0.00      0.00      0.00       105\n",
      "              refugees       0.50      0.00      0.01       330\n",
      "                 death       0.00      0.00      0.00       394\n",
      "             other_aid       0.10      0.00      0.01      1095\n",
      "infrastructure_related       0.00      0.00      0.00       568\n",
      "             transport       0.00      0.00      0.00       402\n",
      "             buildings       0.25      0.01      0.01       447\n",
      "           electricity       0.00      0.00      0.00       176\n",
      "                 tools       0.00      0.00      0.00        46\n",
      "             hospitals       0.00      0.00      0.00        96\n",
      "                 shops       0.00      0.00      0.00        42\n",
      "           aid_centers       0.00      0.00      0.00       107\n",
      "  other_infrastructure       0.00      0.00      0.00       386\n",
      "       weather_related       0.55      0.12      0.19      2393\n",
      "                floods       0.33      0.01      0.01       709\n",
      "                 storm       0.47      0.04      0.07       834\n",
      "                  fire       0.00      0.00      0.00        86\n",
      "            earthquake       0.73      0.14      0.23       813\n",
      "                  cold       0.00      0.00      0.00       167\n",
      "         other_weather       0.00      0.00      0.00       423\n",
      "         direct_report       0.41      0.06      0.10      1662\n",
      "\n",
      "           avg / total       0.42      0.25      0.26     27139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "label_columns = df.drop(['id', 'message', 'original', 'genre'], axis=1).columns.tolist()\n",
    "print(classification_report(y_test, y_pred, target_names=label_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Original Pipeline\n",
    "pipeline = Pipeline([('cvectorizer', CountVectorizer(tokenizer=tokenize)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('rf_clf', RandomForestClassifier())\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Specifying parameters for grid search\n",
    "parameters = {'rf_clf__min_samples_leaf':[1, 2],\n",
    "              'rf_clf__min_samples_split': [2, 4],\n",
    "              'rf_clf__n_estimators': [10, 30, 50]}\n",
    "              \n",
    "\n",
    "### Creating grid search object\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "      ...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'rf_clf__min_samples_leaf': [1, 2], 'rf_clf__min_samples_split': [2, 4], 'rf_clf__n_estimators': [10, 30, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Fitting the new model\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf_clf__min_samples_leaf': 1,\n",
       " 'rf_clf__min_samples_split': 2,\n",
       " 'rf_clf__n_estimators': 50}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Checking which parameter values optimize this model \n",
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9263916188127079\n",
      "Precision: 0.41643241958155297\n",
      "Recall: 0.2619477504698036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = cv.predict(X_test)\n",
    "accuracy = (y_pred == y_test).mean() \n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted')}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.77      0.94      0.84      6546\n",
      "               request       0.41      0.05      0.08      1455\n",
      "                 offer       0.00      0.00      0.00        34\n",
      "           aid_related       0.45      0.11      0.18      3516\n",
      "          medical_help       0.00      0.00      0.00       665\n",
      "      medical_products       0.00      0.00      0.00       407\n",
      "     search_and_rescue       0.50      0.00      0.01       234\n",
      "              security       0.00      0.00      0.00       170\n",
      "              military       0.50      0.00      0.01       283\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.00      0.00      0.00       522\n",
      "                  food       0.21      0.00      0.01       909\n",
      "               shelter       0.00      0.00      0.00       784\n",
      "              clothing       0.00      0.00      0.00       136\n",
      "                 money       0.20      0.01      0.01       197\n",
      "        missing_people       0.00      0.00      0.00       105\n",
      "              refugees       0.00      0.00      0.00       330\n",
      "                 death       0.00      0.00      0.00       394\n",
      "             other_aid       0.16      0.00      0.01      1095\n",
      "infrastructure_related       0.00      0.00      0.00       568\n",
      "             transport       0.00      0.00      0.00       402\n",
      "             buildings       0.10      0.00      0.00       447\n",
      "           electricity       0.00      0.00      0.00       176\n",
      "                 tools       0.00      0.00      0.00        46\n",
      "             hospitals       0.00      0.00      0.00        96\n",
      "                 shops       0.00      0.00      0.00        42\n",
      "           aid_centers       0.00      0.00      0.00       107\n",
      "  other_infrastructure       0.00      0.00      0.00       386\n",
      "       weather_related       0.62      0.12      0.20      2393\n",
      "                floods       0.00      0.00      0.00       709\n",
      "                 storm       0.44      0.03      0.05       834\n",
      "                  fire       0.00      0.00      0.00        86\n",
      "            earthquake       0.75      0.16      0.26       813\n",
      "                  cold       0.00      0.00      0.00       167\n",
      "         other_weather       0.00      0.00      0.00       423\n",
      "         direct_report       0.36      0.03      0.06      1662\n",
      "\n",
      "           avg / total       0.40      0.26      0.26     27139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=label_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Original Pipeline\n",
    "pipeline = Pipeline([('cvectorizer', CountVectorizer(tokenizer=tokenize)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('rf_clf', RandomForestClassifier())\n",
    "                    ])\n",
    "\n",
    "\n",
    "### Specifying parameters for grid search\n",
    "parameters2 = {'cvectorizer__ngram_range': ((1, 1), (1, 2)),\n",
    "               'cvectorizer__max_df': (0.5, 0.75, 1.0),\n",
    "               'cvectorizer__max_features': (None, 5000, 10000),\n",
    "               'tfidf__use_idf': (True, False),\n",
    "               'rf_clf__min_samples_leaf':[1, 2],\n",
    "               'rf_clf__min_samples_split': [2, 4],\n",
    "               'rf_clf__n_estimators': [10, 30, 50]}\n",
    "              \n",
    "\n",
    "### Creating a Randomized search object - Grid Search is taking way too long\n",
    "#cv_final = GridSearchCV(pipeline, param_grid=parameters2)\n",
    "cv_final = RandomizedSearchCV(pipeline, param_distributions=parameters2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fitting the new model\n",
    "with active_session():\n",
    "    cv_final.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf__use_idf': False,\n",
       " 'rf_clf__n_estimators': 50,\n",
       " 'rf_clf__min_samples_split': 2,\n",
       " 'rf_clf__min_samples_leaf': 1,\n",
       " 'cvectorizer__ngram_range': (1, 2),\n",
       " 'cvectorizer__max_features': None,\n",
       " 'cvectorizer__max_df': 1.0}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Checking which parameter values optimize this model \n",
    "cv_final.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9259162909006822\n",
      "Precision: 0.4119280688760612\n",
      "Recall: 0.23593352739599838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "### Testing the model\n",
    "y_pred = cv_final.predict(X_test)\n",
    "accuracy = (y_pred == y_test).mean() \n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted')}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.77      0.88      0.82      6546\n",
      "               request       0.42      0.03      0.05      1455\n",
      "                 offer       0.00      0.00      0.00        34\n",
      "           aid_related       0.45      0.05      0.09      3516\n",
      "          medical_help       0.00      0.00      0.00       665\n",
      "      medical_products       0.00      0.00      0.00       407\n",
      "     search_and_rescue       0.00      0.00      0.00       234\n",
      "              security       0.00      0.00      0.00       170\n",
      "              military       0.50      0.00      0.01       283\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.00      0.00      0.00       522\n",
      "                  food       0.21      0.00      0.01       909\n",
      "               shelter       0.08      0.00      0.00       784\n",
      "              clothing       0.00      0.00      0.00       136\n",
      "                 money       0.50      0.01      0.02       197\n",
      "        missing_people       0.00      0.00      0.00       105\n",
      "              refugees       0.00      0.00      0.00       330\n",
      "                 death       0.00      0.00      0.00       394\n",
      "             other_aid       0.11      0.00      0.00      1095\n",
      "infrastructure_related       0.00      0.00      0.00       568\n",
      "             transport       0.00      0.00      0.00       402\n",
      "             buildings       0.20      0.00      0.01       447\n",
      "           electricity       0.00      0.00      0.00       176\n",
      "                 tools       0.00      0.00      0.00        46\n",
      "             hospitals       0.00      0.00      0.00        96\n",
      "                 shops       0.00      0.00      0.00        42\n",
      "           aid_centers       0.00      0.00      0.00       107\n",
      "  other_infrastructure       0.00      0.00      0.00       386\n",
      "       weather_related       0.65      0.09      0.16      2393\n",
      "                floods       0.00      0.00      0.00       709\n",
      "                 storm       0.46      0.02      0.04       834\n",
      "                  fire       0.00      0.00      0.00        86\n",
      "            earthquake       0.75      0.15      0.25       813\n",
      "                  cold       0.00      0.00      0.00       167\n",
      "         other_weather       0.00      0.00      0.00       423\n",
      "         direct_report       0.40      0.02      0.04      1662\n",
      "\n",
      "           avg / total       0.41      0.24      0.24     27139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=label_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disaster_response_model.pkl']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(cv_final.best_estimator_, 'disaster_response_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Downloading necessary NLTK data\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords'])\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "### Importing libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "### Importing packages for the Machine Learning Model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "### Importing packages to save the model\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "### Declaring the error message\n",
    "error_message = '''\n",
    "Please, provide the filepaths of the messages and categories datasets as the first and second arguments,\n",
    "respectively, as well as the path of the database to save the cleansed data as the third argument.\n",
    "Example: python process_data.py disaster_messages.csv disaster_categories.csv DisasterResponse.db\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads the data used in the model - messages.csv and categories.csv\n",
    "    Both files are located in the home directory\n",
    "    \n",
    "    INPUT:  None\n",
    "    \n",
    "    OUTPUT: X  - Numpy array originated from the dataframe saved in the SQL server. It contains the predictor.\n",
    "            y  - Numpy array originated from the dataframe saved in the SQL server. It contains the labels.\n",
    "            df - The dataframe saved in the SQL server from process_data.py  \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Declaring the query and the engine to connect to the database and pull the data from the previous step\n",
    "    print('Loading data')\n",
    "    querystring = \"\"\"SELECT * from disaster_response_mod\"\"\"\n",
    "    engine = create_engine('sqlite:///InsertDatabaseName.db')\n",
    "    \n",
    "    ### Connecting to the database and querying the data\n",
    "    df = pd.read_sql(querystring, engine)\n",
    "\n",
    "    ### Dropping columns and converting to Numpy arrays\n",
    "    X = df[['message']].values\n",
    "    y = df.drop(['id', 'message', 'original', 'genre'], axis=1).values\n",
    "    print('Data is ready to be tokenized')\n",
    "    \n",
    "    return X, y, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    ''''\n",
    "    Tokenizer that gets a string and converts it in tokens, to be used in the Machine Learning model\n",
    "    \n",
    "    INPUT:  text - The original text to be tokenized/lemmatized\n",
    "    OUTPUT: Lemmatized and tokenized text\n",
    "    '''\n",
    "    \n",
    "    ### Getting list of all urls using regex\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    \n",
    "    ### Replacing each url in text string with placeholder\n",
    "    for url in detected_urls:\n",
    "        bbtext = text.replace(url, \"urlplaceholder\")\n",
    "    \n",
    "    ### Tokenizing text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    ### Initiating lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "    ### Lemmatizing, normalizing case, and removing leading/trailing white space and stop words\n",
    "    print('Tokenizer function created')\n",
    "    return[lemmatizer.lemmatize(tok).lower().strip() for tok in tokens if tok not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X, y):\n",
    "    \n",
    "    '''\n",
    "    Splits the data into training and testing sets, created the pipeline and finds the best combination of\n",
    "    hyperparameters to optmize the model\n",
    "    \n",
    "    INPUT:  X  - Numpy array originated from the dataframe saved in the SQL server. It contains the predictor.\n",
    "            y  - Numpy array originated from the dataframe saved in the SQL server. It contains the labels.\n",
    "    \n",
    "    OUTPUT: X_test - Testing data, after running train_test_split. To be used for evalutating the model.\n",
    "            y_test - Teating labels, after running train_test_split. To be used for evaluating the model.\n",
    "            cv     - The optimized model, after performing Grid Search\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ### Performing train test split\n",
    "    print('Splitting the data')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    ### Flattening arrays\n",
    "    X_train = X_train.ravel()\n",
    "    X_test = X_test.ravel()\n",
    "    \n",
    "    ### Building the Pipeline\n",
    "    print('Building the Pipeline')\n",
    "    pipeline = Pipeline([('cvectorizer', CountVectorizer(tokenizer=tokenize)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('rf_clf', RandomForestClassifier())\n",
    "                        ])\n",
    "\n",
    "    ### Specifying parameters for grid search\n",
    "    parameters = {'cvectorizer__ngram_range': ((1, 1), (1, 2)),\n",
    "                  'cvectorizer__max_df': (0.5, 0.75, 1.0),\n",
    "                  'cvectorizer__max_features': (None, 5000, 10000),\n",
    "                  'tfidf__use_idf': (True, False),\n",
    "                  'rf_clf__min_samples_leaf':[1, 2],\n",
    "                  'rf_clf__min_samples_split': [2, 4],\n",
    "                  'rf_clf__n_estimators': [10, 30, 50]}\n",
    "    \n",
    "    ### Creating grid search object\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "    \n",
    "    ### Fitting the model\n",
    "    print('Fitting the model')\n",
    "    cv.fit(X_train, y_train)\n",
    "    \n",
    "    ### Printing best hyperparamaters and returning the model\n",
    "    print('Printing best hyperparamaters and returning the model')\n",
    "    print(cv_final.best_params_)\n",
    "    \n",
    "    return X_test, y_test, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Loads the data used in the model - messages.csv and categories.csv\n",
    "    Both files are located in the home directory\n",
    "    \n",
    "    INPUT: model  - The optimized model, with its hyperparameters optimized after Grid Search\n",
    "           X_test - Testing data, after running train_test_split\n",
    "           y_test - Teating labels, after running train_test_split\n",
    "           df     - The dataframe saved in the SQL server from process_data.py\n",
    "    \n",
    "    OUTPUT: None\n",
    "    \"\"\"\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = (y_pred == y_test).mean() \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred, average='weighted')}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred, average='weighted')}\")\n",
    "\n",
    "    label_columns = df.drop(['id', 'message', 'original', 'genre'], axis=1).columns.tolist()\n",
    "    print(classification_report(y_test, y_pred, target_names=label_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Saves the model to a pickle file\n",
    "    \n",
    "    INPUT:  model      - The optimized model, with its hyperparameters optimized after Grid Search\n",
    "            model_path - The path where the pickle file will be saved\n",
    "    \n",
    "    OUTPUT: None\n",
    "    \"\"\"\n",
    "    \n",
    "    joblib.dump(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if len(sys.argv) == 3:\n",
    "        df1_path, df2_path, db_path = sys.argv[1:]\n",
    "        \n",
    "    print('Calling functions')\n",
    "    X, y, df =  load_data()\n",
    "    X_test, y_test, cv_final = build_model(X, y)\n",
    "    evaluate_model(X_test, y_test, cv_final, df)\n",
    "    save_model(cv_final.best_estimator_, 'disaster_response_model_final.pkl')\n",
    "    print('End of code')\n",
    "        \n",
    "    else:\n",
    "        print(error_message)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
